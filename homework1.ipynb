{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Fashion MNIST dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    \"./data\",\n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor()]),\n",
    ")\n",
    "test_set = torchvision.datasets.FashionMNIST(\n",
    "    \"./data\",\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transforms.Compose([transforms.ToTensor()]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print an example of the dataset to understand the structure of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = next(iter(train_set))\n",
    "plt.imshow(image.squeeze(), cmap=\"gray\")\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "np.random.seed(0)\n",
    "indices = np.random.choice(len(train_set), 2000, replace=False)\n",
    "train_subset = torch.utils.data.Subset(train_set, indices)\n",
    "\n",
    "images_flat = np.array(\n",
    "    [image.flatten().numpy() for image, _ in train_subset], dtype=np.float32\n",
    ")\n",
    "labels = np.array([label for _, label in train_subset])\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(images_flat)\n",
    "\n",
    "scatter = plt.scatter(\n",
    "    pca_result[:, 0], pca_result[:, 1], c=labels, cmap=\"tab10\", alpha=0.7, s=5\n",
    ")\n",
    "colorbar = plt.colorbar(scatter)\n",
    "colorbar.set_ticks(range(10))\n",
    "colorbar.set_ticklabels(train_set.classes)\n",
    "plt.title(\"2-dim PCA of Fashion MNIST\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same with umap\n",
    "import umap\n",
    "\n",
    "reducer = umap.UMAP()\n",
    "umap_result = reducer.fit_transform(images_flat)\n",
    "\n",
    "scatter = plt.scatter(\n",
    "    umap_result[:, 0], umap_result[:, 1], c=labels, cmap=\"tab10\", alpha=0.7, s=5\n",
    ")\n",
    "colorbar = plt.colorbar(scatter)\n",
    "colorbar.set_ticks(range(10))\n",
    "colorbar.set_ticklabels(train_set.classes)\n",
    "plt.title(\"2-dim UMAP of Fashion MNIST\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "tsne_result = tsne.fit_transform(images_flat)\n",
    "\n",
    "scatter = plt.scatter(\n",
    "    tsne_result[:, 0], tsne_result[:, 1], c=labels, cmap=\"tab10\", alpha=0.7, s=5\n",
    ")\n",
    "colorbar = plt.colorbar(scatter)\n",
    "colorbar.set_ticks(range(10))\n",
    "colorbar.set_ticklabels(train_set.classes)\n",
    "plt.title(\"2-dim t-SNE of Fashion MNIST\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    One residual layer inputs:\n",
    "    - in_dim : the input dimension\n",
    "    - res_h_dim : the hidden dimension of the residual block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, res_h_dim):\n",
    "        super(ResidualLayer, self).__init__()\n",
    "\n",
    "        self.res_block = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_dim, res_h_dim, kernel_size=3, stride=1, padding=1, bias=False\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(res_h_dim, in_dim, kernel_size=1, stride=1, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.res_block(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResidualStack(nn.Module):\n",
    "    \"\"\"\n",
    "    A stack of residual layers inputs:\n",
    "    - in_dim : the input dimension\n",
    "    - res_h_dim : the hidden dimension of the residual block\n",
    "    - n_res_layers : number of layers to stack\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, res_h_dim, n_res_layers):\n",
    "        super(ResidualStack, self).__init__()\n",
    "\n",
    "        self.n_res_layers = n_res_layers\n",
    "        self.stack = nn.ModuleList(\n",
    "            [ResidualLayer(in_dim, res_h_dim) for _ in range(n_res_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.stack:\n",
    "            x = layer(x)\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_channels, hidden_dim, output_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                n_channels, hidden_dim // 2, kernel_size=4, stride=2, padding=1\n",
    "            ),  # Input: [n_channels, 28, 28] Output: [hidden_dim // 2, 14, 14]\n",
    "            nn.BatchNorm2d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                hidden_dim // 2, hidden_dim, kernel_size=4, stride=2, padding=1\n",
    "            ),  # Input: [hidden_dim // 2, 14, 14] Output: [hidden_dim, 7, 7]\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                hidden_dim, hidden_dim, kernel_size=3, stride=1, padding=1\n",
    "            ),  # Input: [hidden_dim, 7, 7] Output: [hidden_dim, 7, 7]\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            ResidualStack(\n",
    "                hidden_dim, hidden_dim, hidden_dim, 2\n",
    "            ),  # Input: [hidden_dim, 7, 7] Output: [hidden_dim, 7, 7]\n",
    "            nn.Conv2d(\n",
    "                hidden_dim, output_dim, kernel_size=3, stride=1, padding=1\n",
    "            ),  # Input: [hidden_dim, 7, 7] Output: [output_dim, 7, 7]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        # the dictionary of embeddings\n",
    "        self.embedding = nn.Embedding(self.num_embeddings, self.embedding_dim)\n",
    "        self.embedding.weight.data.uniform_(\n",
    "            -1 / self.num_embeddings, 1 / self.num_embeddings\n",
    "        )\n",
    "        self.commitment_cost = commitment_cost\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor):\n",
    "        # convert inputs from BCHW -> BHWC\n",
    "        inputs = inputs.permute(0, 2, 3, 1).contiguous()\n",
    "        input_shape = inputs.shape\n",
    "\n",
    "        # Flatten input\n",
    "        flat_input = inputs.view(-1, self.embedding_dim)\n",
    "\n",
    "        # Calculate Eucledian distance between input and embeddings\n",
    "        distances = (\n",
    "            torch.sum(flat_input**2, dim=1, keepdim=True)\n",
    "            + torch.sum(self.embedding.weight**2, dim=1)\n",
    "            - 2 * torch.matmul(flat_input, self.embedding.weight.t())\n",
    "        )\n",
    "\n",
    "        # Encoding\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
    "        # one-hot encoding\n",
    "        encodings = torch.zeros(\n",
    "            encoding_indices.shape[0], self.num_embeddings, device=inputs.device\n",
    "        )\n",
    "        encodings.scatter_(1, encoding_indices, 1)\n",
    "\n",
    "        # Quantize and unflatten\n",
    "        quantized = torch.matmul(encodings, self.embedding.weight).view(input_shape)\n",
    "\n",
    "        # Loss\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n",
    "        q_latent_loss = F.mse_loss(quantized, inputs.detach())\n",
    "        loss = q_latent_loss + self.commitment_cost * e_latent_loss\n",
    "\n",
    "        # Straight-through estimator\n",
    "        quantized = inputs + (quantized - inputs).detach()\n",
    "\n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "\n",
    "        # convert quantized from BHWC -> BCHW\n",
    "        return loss, quantized.permute(0, 3, 1, 2).contiguous(), perplexity, encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                input_dim, hidden_dim, kernel_size=3, stride=1, padding=1\n",
    "            ),  # Input: [input_dim, 7, 7] Output: [hidden_dim, 7, 7]\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            ResidualStack(\n",
    "                hidden_dim, hidden_dim, hidden_dim, 2\n",
    "            ),  # Input: [hidden_dim, 7, 7] Output: [hidden_dim, 7, 7]\n",
    "            nn.ConvTranspose2d(\n",
    "                hidden_dim, hidden_dim // 2, kernel_size=4, stride=2, padding=1\n",
    "            ),  # Input: [hidden_dim, 7, 7] Output: [hidden_dim // 2, 14, 14]\n",
    "            nn.BatchNorm2d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(\n",
    "                hidden_dim // 2, n_channels, kernel_size=4, stride=2, padding=1\n",
    "            ),  # Input: [hidden_dim // 2, 14, 14] Output: [n_channels, 28, 28]\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, n_channels, hidden_dim, num_embeddings, embedding_dim):\n",
    "        super(VQVAE, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            n_channels, hidden_dim, embedding_dim\n",
    "        )  # Input: [n_channels, 28, 28], Output: [embedding_dim, 7, 7]\n",
    "        self.quantizer = VectorQuantizer(\n",
    "            num_embeddings, embedding_dim, 0.25\n",
    "        )  # Input: [embedding_dim, 7, 7], Output: [embedding_dim, 7, 7]\n",
    "        self.decoder = Decoder(\n",
    "            embedding_dim, hidden_dim, n_channels\n",
    "        )  # Input: [embedding_dim, 7, 7], Output: [n_channels, 28, 28]\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        loss, quantized, perplexity, _ = self.quantizer(z)\n",
    "        x_recon = self.decoder(quantized)\n",
    "\n",
    "        return x_recon, loss, perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channels = next(iter(train_set))[0].shape[0]\n",
    "HIDDEN_DIM = 64\n",
    "NUM_EMBEDDINGS = 512\n",
    "EMBEDDING_DIM = 64\n",
    "\n",
    "model = VQVAE(n_channels, HIDDEN_DIM, NUM_EMBEDDINGS, EMBEDDING_DIM).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_args):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=train_args[\"lr\"])\n",
    "\n",
    "    for i, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x_recon, loss, perplexity = model(data)\n",
    "        recon_error = F.mse_loss(x_recon, data)\n",
    "        loss = recon_error + loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0 and train_args[\"verbose\"] == True:\n",
    "            print(\n",
    "                f\"Step: {i}, Loss: {loss.item()}, Recon Error: {recon_error.item()}, Perplexity: {perplexity.item()}\"\n",
    "            )\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "learning_rates = [1e-4]  # [1e-3, 1e-4, 1e-5]\n",
    "best_lr = None\n",
    "best_loss = float(\"inf\")\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"Training with learning rate: {lr}\")\n",
    "    for e in range(epochs):\n",
    "        print(f\"Epoch: {e}\")\n",
    "        train_loss = train({\"lr\": lr, \"verbose\": False})\n",
    "\n",
    "        if train_loss < best_loss:\n",
    "            best_loss = train_loss\n",
    "            best_lr = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (data, _) in enumerate(test_loader):\n",
    "        data = data.to(device)\n",
    "        x_recon, _, _ = model(data)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 8, figsize=(16, 4))\n",
    "fig.tight_layout()\n",
    "\n",
    "indices = torch.randint(0, 100, (8,))\n",
    "for i, idx in enumerate(indices):\n",
    "    axs[0, i].imshow(data[idx].squeeze().cpu(), cmap=\"gray\")\n",
    "    axs[1, i].imshow(x_recon[idx].squeeze().cpu(), cmap=\"gray\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "ssim_val = ssim(\n",
    "    data.cpu().numpy().squeeze(1), x_recon.cpu().numpy().squeeze(1), data_range=1.0\n",
    ")\n",
    "mse_val = mean_squared_error(\n",
    "    data.cpu().numpy().flatten(), x_recon.cpu().numpy().flatten()\n",
    ")\n",
    "\n",
    "print(f\"SSIM: {ssim_val}, MSE: {mse_val}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
