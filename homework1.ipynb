{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Fashion MNIST dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    \"./data\",\n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor()]),\n",
    ")\n",
    "test_set = torchvision.datasets.FashionMNIST(\n",
    "    \"./data\",\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transforms.Compose([transforms.ToTensor()]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize an sample of the dataset to understand the structure of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, _ = next(iter(train_set))\n",
    "plt.imshow(image.squeeze(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here instead we print some initial information about the dataset classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Classes: {train_set.classes}\")\n",
    "print(f\"Number of classes: {len(train_set.classes)}\")\n",
    "print(f\"Distribution of classes: {np.bincount(train_set.targets)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the dataset is composed of images of 28x28 pixels in grayscale.\n",
    "\n",
    "The images are divided into 10 classes, each representing a different type of clothing.\n",
    "Moreover the classes are balanced, with 6000 images for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will perform some exploratory data analysis to understand the structure of the data.\n",
    "\n",
    "We will use dimensionality reduction techniques to visualize the data in 2D.\n",
    "\n",
    "We expect to see clusters of similar classes: for example, shoes and sandals should be close to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "np.random.seed(0)\n",
    "indices = np.random.choice(len(train_set), 2000, replace=False)\n",
    "train_subset = torch.utils.data.Subset(train_set, indices)\n",
    "\n",
    "images_flat = np.array(\n",
    "    [image.flatten().numpy() for image, _ in train_subset], dtype=np.float32\n",
    ")\n",
    "labels = np.array([label for _, label in train_subset])\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(images_flat)\n",
    "\n",
    "scatter = plt.scatter(\n",
    "    pca_result[:, 0], pca_result[:, 1], c=labels, cmap=\"tab10\", alpha=0.7, s=5\n",
    ")\n",
    "colorbar = plt.colorbar(scatter)\n",
    "colorbar.set_ticks(range(10))\n",
    "colorbar.set_ticklabels(train_set.classes)\n",
    "plt.title(\"2-dim PCA of Fashion MNIST\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the plot, the classes are not well separated. This could be due to the fact that taking only 2 dimensions is not enough to separate the classes.\n",
    "\n",
    "We will try to use more advanced techniques to visualize the data in 2D such as UMAP and t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "reducer = umap.UMAP()\n",
    "umap_result = reducer.fit_transform(images_flat)\n",
    "\n",
    "scatter = plt.scatter(\n",
    "    umap_result[:, 0], umap_result[:, 1], c=labels, cmap=\"tab10\", alpha=0.7, s=5\n",
    ")\n",
    "colorbar = plt.colorbar(scatter)\n",
    "colorbar.set_ticks(range(10))\n",
    "colorbar.set_ticklabels(train_set.classes)\n",
    "plt.title(\"2-dim UMAP of Fashion MNIST\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "tsne_result = tsne.fit_transform(images_flat)\n",
    "\n",
    "scatter = plt.scatter(\n",
    "    tsne_result[:, 0], tsne_result[:, 1], c=labels, cmap=\"tab10\", alpha=0.7, s=5\n",
    ")\n",
    "colorbar = plt.colorbar(scatter)\n",
    "colorbar.set_ticks(range(10))\n",
    "colorbar.set_ticklabels(train_set.classes)\n",
    "plt.title(\"2-dim t-SNE of Fashion MNIST\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we can see that the classes are better separated. Both UMAP and t-SNE are able to separate the classes in a more meaningful way.\n",
    "\n",
    "There are 4 main clusters: shoes, shirts, trousers and bags.\n",
    "\n",
    "When we will analyze the latent space, we epect to see similar clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    One residual layer inputs:\n",
    "    - in_dim : the input dimension\n",
    "    - res_h_dim : the hidden dimension of the residual block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, res_h_dim):\n",
    "        super(ResidualLayer, self).__init__()\n",
    "\n",
    "        self.res_block = nn.Sequential(\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                in_dim, res_h_dim, kernel_size=3, stride=1, padding=1, bias=False\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(res_h_dim, in_dim, kernel_size=1, stride=1, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.res_block(x)\n",
    "\n",
    "\n",
    "class ResidualStack(nn.Module):\n",
    "    \"\"\"\n",
    "    A stack of residual layers inputs:\n",
    "    - in_dim : the input dimension\n",
    "    - res_h_dim : the hidden dimension of the residual block\n",
    "    - n_res_layers : number of layers to stack\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, res_h_dim, n_res_layers):\n",
    "        super(ResidualStack, self).__init__()\n",
    "\n",
    "        self.n_res_layers = n_res_layers\n",
    "        self.stack = nn.ModuleList(\n",
    "            [ResidualLayer(in_dim, res_h_dim) for _ in range(n_res_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.stack:\n",
    "            x = layer(x)\n",
    "        return F.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder module for extracting latent representations from input data.\n",
    "\n",
    "    Args:\n",
    "        n_channels (int): Number of input channels (e.g., 1 for grayscale images, 3 for RGB).\n",
    "        hidden_dim (int): Dimension of hidden layers in the encoder.\n",
    "        output_dim (int): Dimension of the latent representation.\n",
    "        n_res_layers (int): Number of residual layers to use in the encoder.\n",
    "\n",
    "    Architecture:\n",
    "        - Three convolutional layers with downsampling and ReLU activations.\n",
    "        - One residual stack for feature refinement.\n",
    "        - A final convolutional layer projecting to the latent space.\n",
    "\n",
    "    Input:\n",
    "        torch.Tensor: Input tensor of shape (batch_size, n_channels, height, width).\n",
    "\n",
    "    Output:\n",
    "        torch.Tensor: Latent representation tensor of shape (batch_size, output_dim, height // 4, width // 4).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels, hidden_dim, output_dim, n_res_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                n_channels, hidden_dim // 2, kernel_size=4, stride=2, padding=1\n",
    "            ),  # Input: [n_channels, 28, 28] Output: [hidden_dim // 2, 14, 14]\n",
    "            nn.BatchNorm2d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                hidden_dim // 2, hidden_dim, kernel_size=4, stride=2, padding=1\n",
    "            ),  # Input: [hidden_dim // 2, 14, 14] Output: [hidden_dim, 7, 7]\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                hidden_dim, hidden_dim, kernel_size=3, stride=1, padding=1\n",
    "            ),  # Input: [hidden_dim, 7, 7] Output: [hidden_dim, 7, 7]\n",
    "            # nn.BatchNorm2d(hidden_dim),\n",
    "            # nn.ReLU(),\n",
    "            ResidualStack(\n",
    "                hidden_dim, hidden_dim, n_res_layers\n",
    "            ),  # Input: [hidden_dim, 7, 7] Output: [hidden_dim, 7, 7]\n",
    "            # nn.Conv2d(\n",
    "            #     hidden_dim, output_dim, kernel_size=3, stride=1, padding=1\n",
    "            # ),  # Input: [hidden_dim, 7, 7] Output: [output_dim, 7, 7]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used https://shashank7-iitd.medium.com/understanding-vector-quantized-variational-autoencoders-vq-vae-323d710a888a\n",
    "# as a reference for the implementation of the VectorQuantizer class\n",
    "\n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "    \"\"\"\n",
    "    Vector Quantizer module for discretizing latent representations using a codebook.\n",
    "\n",
    "    Args:\n",
    "        num_embeddings (int): Number of embeddings in the codebook.\n",
    "        embedding_dim (int): Dimension of each embedding vector.\n",
    "        commitment_cost (float): Weight of the commitment loss, ensuring latent codes match inputs.\n",
    "\n",
    "    Forward Method:\n",
    "        - Flattens input data and computes distances between inputs and embedding vectors.\n",
    "        - Finds the nearest embedding for each input and reconstructs the quantized representation.\n",
    "        - Computes the quantization loss and perplexity of the embedding usage.\n",
    "\n",
    "    Input:\n",
    "        torch.Tensor: Input tensor of shape (batch_size, channels, height, width).\n",
    "\n",
    "    Output:\n",
    "        Tuple:\n",
    "            - loss (torch.Tensor): Quantization loss.\n",
    "            - quantized (torch.Tensor): Quantized output tensor of shape (batch_size, channels, height, width).\n",
    "            - perplexity (torch.Tensor): Measure of the diversity of embeddings used.\n",
    "            - encodings (torch.Tensor): One-hot encoding of the selected embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        # the dictionary of embeddings\n",
    "        self.embedding = nn.Embedding(self.num_embeddings, self.embedding_dim)\n",
    "        self.embedding.weight.data.uniform_(\n",
    "            -1 / self.num_embeddings, 1 / self.num_embeddings\n",
    "        )\n",
    "        self.commitment_cost = commitment_cost\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor):\n",
    "        inputs = inputs.permute(0, 2, 3, 1).contiguous()\n",
    "        input_shape = inputs.shape\n",
    "\n",
    "        # Flatten input\n",
    "        flat_input = inputs.view(-1, self.embedding_dim)\n",
    "\n",
    "        # Calculate Eucledian distance between input and embeddings\n",
    "        distances = (\n",
    "            torch.sum(flat_input**2, dim=1, keepdim=True)\n",
    "            + torch.sum(self.embedding.weight**2, dim=1)\n",
    "            - 2 * torch.matmul(flat_input, self.embedding.weight.t())\n",
    "        )\n",
    "\n",
    "        # Encoding\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
    "        # one-hot encoding\n",
    "        encodings = torch.zeros(\n",
    "            encoding_indices.shape[0], self.num_embeddings, device=inputs.device\n",
    "        )\n",
    "        encodings.scatter_(1, encoding_indices, 1)\n",
    "\n",
    "        # Quantize and unflatten\n",
    "        quantized = torch.matmul(encodings, self.embedding.weight).view(input_shape)\n",
    "\n",
    "        # Loss\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n",
    "        q_latent_loss = F.mse_loss(quantized, inputs.detach())\n",
    "        loss = q_latent_loss + self.commitment_cost * e_latent_loss\n",
    "\n",
    "        # Straight-through estimator\n",
    "        quantized = inputs + (quantized - inputs).detach()\n",
    "\n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "\n",
    "        # convert quantized from BHWC -> BCHW\n",
    "        return loss, quantized.permute(0, 3, 1, 2).contiguous(), perplexity, encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder module for reconstructing input data from latent representations.\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): Dimension of the input latent representation.\n",
    "        hidden_dim (int): Dimension of hidden layers in the decoder.\n",
    "        n_channels (int): Number of output channels (e.g., 1 for grayscale images, 3 for RGB).\n",
    "        n_res_layers (int): Number of residual layers to use in the decoder.\n",
    "\n",
    "    Architecture:\n",
    "        - One transposed convolutional layer to match the latent input shape.\n",
    "        - One residual stack for feature refinement.\n",
    "        - Two transposed convolutional layers for upsampling.\n",
    "        - A final sigmoid activation for output normalization (range: [0, 1]).\n",
    "\n",
    "    Input:\n",
    "        torch.Tensor: Latent representation tensor of shape (batch_size, input_dim, height, width).\n",
    "\n",
    "    Output:\n",
    "        torch.Tensor: Reconstructed data tensor of shape (batch_size, n_channels, height * 4, width * 4).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, n_channels, n_res_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            # nn.ConvTranspose2d(\n",
    "            #     input_dim, hidden_dim, kernel_size=3, stride=1, padding=1\n",
    "            # ),  # Input: [input_dim, 7, 7] Output: [hidden_dim, 7, 7]\n",
    "            # nn.BatchNorm2d(hidden_dim),\n",
    "            # nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                input_dim, hidden_dim, kernel_size=3, stride=1, padding=1\n",
    "            ),  # Input: [input_dim, 7, 7] Output: [hidden_dim, 7, 7]\n",
    "            ResidualStack(\n",
    "                hidden_dim, hidden_dim, n_res_layers\n",
    "            ),  # Input: [hidden_dim, 7, 7] Output: [hidden_dim, 7, 7]\n",
    "            nn.ConvTranspose2d(\n",
    "                hidden_dim, hidden_dim // 2, kernel_size=4, stride=2, padding=1\n",
    "            ),  # Input: [hidden_dim, 7, 7] Output: [hidden_dim // 2, 14, 14]\n",
    "            nn.BatchNorm2d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(\n",
    "                hidden_dim // 2, n_channels, kernel_size=4, stride=2, padding=1\n",
    "            ),  # Input: [hidden_dim // 2, 14, 14] Output: [n_channels, 28, 28]\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Vector Quantized Variational Autoencoder (VQVAE) for learning discrete latent representations.\n",
    "\n",
    "    This model consists of three main components:\n",
    "        1. Encoder: Maps the input data to a continuous latent space.\n",
    "        2. Vector Quantizer: Discretizes the latent representations using a learned codebook.\n",
    "        3. Decoder: Reconstructs the input data from the quantized latent representations.\n",
    "\n",
    "    Args:\n",
    "        n_channels (int): Number of input channels (e.g., 1 for grayscale, 3 for RGB).\n",
    "        hidden_dim (int): Dimension of hidden layers in the encoder and decoder.\n",
    "        num_embeddings (int): Number of embeddings in the quantization codebook.\n",
    "        embedding_dim (int): Dimension of each embedding vector in the codebook.\n",
    "        n_res_layers (int): Number of residual layers in the encoder and decoder.\n",
    "        commitment_cost (float): Weight of the commitment loss in the quantization loss.\n",
    "\n",
    "    Forward Method:\n",
    "        - Encodes the input into a latent representation.\n",
    "        - Quantizes the latent representation using the vector quantizer.\n",
    "        - Reconstructs the input from the quantized latent representation.\n",
    "        - Returns the reconstructed input, quantization loss, and perplexity.\n",
    "\n",
    "    Input:\n",
    "        torch.Tensor: Input tensor of shape (batch_size, n_channels, height, width).\n",
    "\n",
    "    Output:\n",
    "        Tuple:\n",
    "            - x_recon (torch.Tensor): Reconstructed tensor of shape (batch_size, n_channels, height, width).\n",
    "            - loss (torch.Tensor): Quantization loss.\n",
    "            - perplexity (torch.Tensor): Measure of the diversity of embeddings used.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_channels,\n",
    "        hidden_dim,\n",
    "        num_embeddings,\n",
    "        embedding_dim,\n",
    "        n_res_layers,\n",
    "        commitment_cost,\n",
    "    ):\n",
    "        super(VQVAE, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            n_channels, hidden_dim, embedding_dim, n_res_layers\n",
    "        )  # Input: [n_channels, 28, 28], Output: [embedding_dim, 7, 7]\n",
    "        self.pre_vq_conv = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim, embedding_dim, kernel_size=1, stride=1)\n",
    "        )  # Input: [embedding_dim, 7, 7], Output: [embedding_dim, 7, 7]\n",
    "        self.quantizer = VectorQuantizer(\n",
    "            num_embeddings, embedding_dim, commitment_cost\n",
    "        )  # Input: [embedding_dim, 7, 7], Output: [embedding_dim, 7, 7]\n",
    "        self.decoder = Decoder(\n",
    "            embedding_dim, hidden_dim, n_channels, n_res_layers\n",
    "        )  # Input: [embedding_dim, 7, 7], Output: [n_channels, 28, 28]\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z = self.pre_vq_conv(z)\n",
    "        loss, quantized, perplexity, _ = self.quantizer(z)\n",
    "        x_recon = self.decoder(quantized)\n",
    "\n",
    "        return x_recon, loss, perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channels = next(iter(train_set))[0].shape[0]\n",
    "HIDDEN_DIM = 128\n",
    "NUM_EMBEDDINGS = 512\n",
    "EMBEDDING_DIM = 64\n",
    "n_res_layers = 2\n",
    "commitment_cost = 0.25\n",
    "\n",
    "model = VQVAE(\n",
    "    n_channels, HIDDEN_DIM, NUM_EMBEDDINGS, EMBEDDING_DIM, n_res_layers, commitment_cost\n",
    ").to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(optimizer, verbose):\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    perplexity_list = []\n",
    "\n",
    "    for i, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x_recon, loss, perplexity = model(data)\n",
    "        recon_error = F.mse_loss(x_recon, data)\n",
    "        loss = recon_error + loss\n",
    "\n",
    "        perplexity_list.append(perplexity.item())\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0 and verbose == True:\n",
    "            print(\n",
    "                f\"Step: {i}, Loss: {loss.item()}, Recon Error: {recon_error.item()}, Perplexity: {perplexity.item()}\"\n",
    "            )\n",
    "\n",
    "    return loss_list, perplexity_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train the model for 10 epochs and search for the best learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "learning_rates = [1e-4]  # You can try multiple values [1e-3, 1e-4, 1e-5]\n",
    "best_lr = None\n",
    "best_loss = float(\"inf\")\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"Training with learning rate: {lr}\")\n",
    "    model = VQVAE(\n",
    "        n_channels,\n",
    "        HIDDEN_DIM,\n",
    "        NUM_EMBEDDINGS,\n",
    "        EMBEDDING_DIM,\n",
    "        n_res_layers,\n",
    "        commitment_cost,\n",
    "    ).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        loss_list, _ = train(optimizer, verbose=False)\n",
    "        print(f\"Epoch: {e} - Loss: {loss_list[-1]}\")\n",
    "\n",
    "        if loss_list[-1] < best_loss:\n",
    "            best_loss = loss_list[-1]\n",
    "            best_lr = lr\n",
    "\n",
    "print(f\"Best learning rate: {best_lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VQVAE(\n",
    "    n_channels, HIDDEN_DIM, NUM_EMBEDDINGS, EMBEDDING_DIM, n_res_layers, commitment_cost\n",
    ").to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_lr)\n",
    "\n",
    "tot_perplexity_list = []\n",
    "tot_loss_list = []\n",
    "\n",
    "for e in range(epochs):\n",
    "    print(f\"Epoch: {e}\")\n",
    "    epoch_loss_list, epoch_perplexity_list = train(optimizer, verbose=True)\n",
    "    tot_perplexity_list.extend(epoch_perplexity_list)\n",
    "    tot_loss_list.extend(epoch_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax[0].plot(tot_loss_list)\n",
    "ax[0].set_title(\"Loss\")\n",
    "ax[0].set_xlabel(\"Step\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[0].set_yscale(\"log\")\n",
    "\n",
    "ax[1].plot(tot_perplexity_list)\n",
    "ax[1].set_title(\"Perplexity\")\n",
    "ax[1].set_xlabel(\"Step\")\n",
    "ax[1].set_ylabel(\"Perplexity\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation the model on the test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained the model, we can evaluate it on the test set.\n",
    "\n",
    "We will compute the reconstruction error and the structural similarity index (SSIM) between the original and reconstructed images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model.eval()\n",
    "ssim_vals = []\n",
    "mse_vals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (data, _) in enumerate(test_loader):\n",
    "        data = data.to(device)\n",
    "        x_recon, _, _ = model(data)\n",
    "\n",
    "        data_np = data.cpu().numpy().squeeze(1)\n",
    "        recon_np = x_recon.cpu().numpy().squeeze(1)\n",
    "\n",
    "        for j in range(data_np.shape[0]):\n",
    "            ssim_score = ssim(data_np[j], recon_np[j], data_range=1.0)\n",
    "            mse_score = mean_squared_error(data_np[j].flatten(), recon_np[j].flatten())\n",
    "\n",
    "            ssim_vals.append(ssim_score)\n",
    "            mse_vals.append(mse_score)\n",
    "\n",
    "avg_ssim = np.mean(ssim_vals)\n",
    "avg_mse = np.mean(mse_vals)\n",
    "\n",
    "print(f\"Average SSIM: {avg_ssim:.4f}, Average MSE: {avg_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 8, figsize=(16, 4))\n",
    "fig.tight_layout()\n",
    "\n",
    "indices = torch.randint(0, 100, (8,))\n",
    "for i, idx in enumerate(indices):\n",
    "    axs[0, i].imshow(data[idx].squeeze().cpu(), cmap=\"gray\")\n",
    "    axs[1, i].imshow(x_recon[idx].squeeze().cpu(), cmap=\"gray\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the model is able to reconstruct the images with a low error and high SSIM.\n",
    "\n",
    "This means that the model is able to capture the structure of the images and reconstruct them with high fidelity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysys of the latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by analyzing the continuous latent space (e.g. the output of the encoder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "latent_space = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        z = model.encoder(data)\n",
    "        latent_space.append(z)\n",
    "\n",
    "latent_space = (\n",
    "    torch.cat(latent_space, dim=0).cpu().numpy().reshape(-1, EMBEDDING_DIM * 7 * 7)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "indices = np.random.choice(len(latent_space), 2000, replace=False)\n",
    "latent_space_subset = latent_space[indices]\n",
    "sampled_labels = [train_set.targets[idx] for idx in indices]\n",
    "labels = np.array(sampled_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(latent_space_subset)\n",
    "\n",
    "scatter = plt.scatter(\n",
    "    pca_result[:, 0], pca_result[:, 1], c=labels, cmap=\"tab10\", alpha=0.7, s=5\n",
    ")\n",
    "colorbar = plt.colorbar(scatter)\n",
    "colorbar.set_ticks(range(10))\n",
    "colorbar.set_ticklabels(train_set.classes)\n",
    "plt.title(\"2-dim PCA of latent space\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP()\n",
    "umap_result = reducer.fit_transform(latent_space_subset)\n",
    "\n",
    "scatter = plt.scatter(\n",
    "    umap_result[:, 0], umap_result[:, 1], c=labels, cmap=\"tab10\", alpha=0.7, s=5\n",
    ")\n",
    "colorbar = plt.colorbar(scatter)\n",
    "colorbar.set_ticks(range(10))\n",
    "colorbar.set_ticklabels(train_set.classes)\n",
    "plt.title(\"2-dim UMAP of latent space\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2)\n",
    "tsne_result = tsne.fit_transform(latent_space_subset)\n",
    "\n",
    "scatter = plt.scatter(\n",
    "    tsne_result[:, 0], tsne_result[:, 1], c=labels, cmap=\"tab10\", alpha=0.7, s=5\n",
    ")\n",
    "colorbar = plt.colorbar(scatter)\n",
    "colorbar.set_ticks(range(10))\n",
    "colorbar.set_ticklabels(train_set.classes)\n",
    "plt.title(\"2-dim t-SNE of latent space\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "discrete_latent_space = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, _ in train_loader:\n",
    "        data = data.to(device)\n",
    "        _, _, _, encodings = model.quantizer(model.encoder(data))\n",
    "        indices = torch.argmax(encodings, dim=1).cpu().numpy()\n",
    "        discrete_latent_space.append(indices)\n",
    "\n",
    "discrete_latent_space = np.concatenate(discrete_latent_space, axis=0).reshape(\n",
    "    len(discrete_latent_space), -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "indices = np.random.choice(\n",
    "    len(discrete_latent_space), len(discrete_latent_space), replace=False\n",
    ")\n",
    "discrete_latent_space_subset = discrete_latent_space[indices]\n",
    "sampled_labels = [train_set.targets[idx] for idx in indices]\n",
    "labels = np.array(sampled_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(n_neighbors=3, min_dist=0.1, metric=\"cosine\")\n",
    "umap_result = reducer.fit_transform(discrete_latent_space_subset)\n",
    "\n",
    "scatter = plt.scatter(\n",
    "    umap_result[:, 0], umap_result[:, 1], c=labels, cmap=\"tab10\", alpha=0.7, s=5\n",
    ")\n",
    "colorbar = plt.colorbar(scatter)\n",
    "colorbar.set_ticks(range(10))\n",
    "colorbar.set_ticklabels(train_set.classes)\n",
    "plt.title(\"2-dim UMAP of discrete latent space\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
